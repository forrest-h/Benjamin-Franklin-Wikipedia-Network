{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Network Analysis\n",
    "## Full Project by Forrest Hangen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import igraph as ig\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Training Model to Identify People in Wikipedia Article Summary\n",
    "(This will use the data and model I developed in Text Classification - Identifying People.ipynb. I did not label all the data I gathered (60000+ article summaries, but if anyone wants to continue labeling, be my guest!))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('wiki_desc_dataset_no_duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32715462610899876"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label['LABEL'].value_counts().loc['PERSON'] /sum(df_label['LABEL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Charles H. Percy</td>\n",
       "      <td>Charles Harting Percy (September 27, 1919 – Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Head of state</td>\n",
       "      <td>A head of state (or chief of state) is the pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Henry Winkler</td>\n",
       "      <td>Henry Franklin Winkler (born October 30, 1945)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>La Violencia</td>\n",
       "      <td>La Violencia (Spanish pronunciation: [la βjoˈl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>London Buses route 176</td>\n",
       "      <td>London Buses route 176 is a Transport for Lond...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL                  Titles  \\\n",
       "0      1        Charles H. Percy   \n",
       "1      0           Head of state   \n",
       "2      1           Henry Winkler   \n",
       "3      0            La Violencia   \n",
       "4      0  London Buses route 176   \n",
       "\n",
       "                                                Text  \n",
       "0  Charles Harting Percy (September 27, 1919 – Se...  \n",
       "1  A head of state (or chief of state) is the pub...  \n",
       "2  Henry Franklin Winkler (born October 30, 1945)...  \n",
       "3  La Violencia (Spanish pronunciation: [la βjoˈl...  \n",
       "4  London Buses route 176 is a Transport for Lond...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labled = pd.read_csv('wiki_desc_dataset_no_duplicates.csv')\n",
    "df_labled_only = (df_labled.dropna()\n",
    "                            .drop(columns= 'Unnamed: 0'))\n",
    "df_labled_only['LABEL'] = df_labled_only['LABEL'].apply(lambda x: 1 if x == 'PERSON' else 0)\n",
    "df_labled_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6122, 190)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_labled_only['Text'], \n",
    "                                                    df_labled_only['LABEL'], \n",
    "                                                    random_state=42, train_size= .97)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_final = CountVectorizer(lowercase=False, min_df = 30, ngram_range=(1,3)).fit(X_train)\n",
    "\n",
    "X_train_matrix = count_final.transform(X_train)\n",
    "X_test_matrix = count_final.transform(X_test)\n",
    "\n",
    "log_final = LogisticRegression(solver = 'newton-cg', C=.1).fit(X_train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Log Reg:\n",
      "Accuracy: 0.9942829140803658\n",
      "ROC AUC: 0.9925539222953105\n",
      "[[4106   10]\n",
      " [  25 1981]]\n",
      "\n",
      "TEST\n",
      "Log Reg:\n",
      "Accuracy: 0.9789473684210527\n",
      "ROC AUC: 0.9847328244274809\n",
      "[[127   4]\n",
      " [  0  59]]\n"
     ]
    }
   ],
   "source": [
    "y_train_finalpred = log_final.predict(X_train_matrix)\n",
    "y_test_finalpred = log_final.predict(X_test_matrix)\n",
    "\n",
    "print('TRAIN')\n",
    "print('Log Reg:\\nAccuracy: {0}\\nROC AUC: {1}'.format(accuracy_score(y_train, y_train_finalpred),\n",
    "                                                     roc_auc_score(y_train, y_train_finalpred)))\n",
    "print(confusion_matrix(y_train, y_train_finalpred))\n",
    "print()\n",
    "print('TEST')\n",
    "print('Log Reg:\\nAccuracy: {0}\\nROC AUC: {1}'.format(accuracy_score(y_test, y_test_finalpred),\n",
    "                                                     roc_auc_score(y_test, y_test_finalpred)))\n",
    "print(confusion_matrix(y_test, y_test_finalpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This level of accuracy is perfect for what I'm trying to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Gathering the Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = parse_page('Benjamin Franklin', all_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON APP_BF\n",
      "ON NEXT LEVELbf: 93/93                                                                                                    \n",
      " \n",
      "\n",
      "\n",
      "Wall time: 51min 12s 1265/1265                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_info = gather_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_all_data():\n",
    "    approved = []\n",
    "    person_links_db = {}\n",
    "    all_checked = []\n",
    "    got_links = []\n",
    "    api_call = \"http://en.wikipedia.org/w/api.php?action=parse&format=json&page=\"\n",
    "\n",
    "    all_info = {'app': approved, 'pldb':person_links_db, 'all':all_checked, 'api':api_call, 'got':got_links}\n",
    "    \n",
    "    all_info = parse_page('Benjamin Franklin', all_info)\n",
    "    \n",
    "    app_bf = all_info['app'].copy()\n",
    "    print('ON APP_BF')\n",
    "    for i, person in enumerate(app_bf):\n",
    "        if person not in all_info['got']:\n",
    "            all_info = parse_page(person, all_info)\n",
    "        print('Finished app_bf: {0}/{1}{2}'.format(i+1, len(app_bf), ' '*100), end=\"\\r\")\n",
    "\n",
    "\n",
    "    next_level = all_info['app'].copy()\n",
    "    print('ON NEXT LEVEL')\n",
    "    print(' ', end ='\\n')\n",
    "    print('\\n')\n",
    "    for i, person in enumerate(next_level):\n",
    "        if person not in all_info['got']:\n",
    "            all_info = parse_page(person, all_info)\n",
    "    print('Finished next level: {0}/{1}{2}'.format(i+1, len(next_level), ' '*100), end=\"\\r\")\n",
    "    return all_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved = []\n",
    "rejected = []\n",
    "links_db = {}\n",
    "person_links_db = {}\n",
    "all_checked = []\n",
    "api_call = \"http://en.wikipedia.org/w/api.php?action=parse&format=json&page=\"\n",
    "\n",
    "all_info = {'app': approved, 'rej':rejected, 'ldb':links_db, 'pldb':person_links_db, 'all':all_checked, 'api':api_call}\n",
    "\n",
    "\n",
    "def parse_page(page_title, all_info):\n",
    "        # Get page and parse\n",
    "        page = requests.get(all_info['api'] + page_title)\n",
    "        source = bs(page.json()['parse']['text']['*'], 'html.parser')\n",
    "\n",
    "        # Get all links\n",
    "        all_links = []\n",
    "        to_check = []\n",
    "        p = source.find_all('p')\n",
    "\n",
    "        for para in p:\n",
    "            p_source = bs(str(para), 'html.parser')\n",
    "            links = p_source.find_all('a', href = True)\n",
    "            for link in links:\n",
    "                title = link.get('title')\n",
    "                # Don't include None\n",
    "                if title is not None:\n",
    "                    all_links.append(title)\n",
    "                    # Check title\n",
    "                    if title not in all_info['all']:\n",
    "                        to_check.append(title)\n",
    "                        all_info['all'].append(title)\n",
    "\n",
    "        if len(to_check) != 0:\n",
    "            df_all_checked = collect_text(to_check)\n",
    "            if df_all_checked is not None:\n",
    "                try:\n",
    "                    X = df_all_checked['Text']\n",
    "                    X_matrix = count_final.transform(X)\n",
    "                    predictions = log_final.predict(X_matrix)\n",
    "\n",
    "                    df_all_checked['Pred'] = predictions\n",
    "    \n",
    "                    good_titles = df_all_checked[df_all_checked['Pred'] == 1]['Titles']\n",
    "                    good_titles = list(good_titles)\n",
    "                    for i in good_titles:\n",
    "                        all_info['app'].append(i)\n",
    "                except:\n",
    "                    test = 'blank'\n",
    "                    \n",
    "        good_links = []\n",
    "        \n",
    "        for link in all_links:\n",
    "            if link in all_info['app']:\n",
    "                good_links.append(link)\n",
    "                \n",
    "        all_info['pldb'][page_title] = good_links\n",
    "        all_info['got'].append(page_title)\n",
    "\n",
    "        return all_info\n",
    "    \n",
    "def split_into_20_names(links):\n",
    "    \n",
    "    links = list(set(links))\n",
    "    \n",
    "    all_links_lists = []\n",
    "    \n",
    "    for i, v in enumerate(links):\n",
    "        if i == 0:\n",
    "            list1 = []\n",
    "            list1.append(v)\n",
    "        elif i % 20 == 0:\n",
    "            all_links_lists.append(list1)\n",
    "            list1 = []\n",
    "            list1.append(v)\n",
    "        elif (i + 1) == len(links):\n",
    "            list1.append(v)\n",
    "            all_links_lists.append(list1)\n",
    "        else:\n",
    "            list1.append(v)\n",
    "            \n",
    "    return all_links_lists\n",
    "\n",
    "def get_text(names_20):\n",
    "    \n",
    "    titles = '|'.join(names_20)\n",
    "    \n",
    "    response = requests.get(\n",
    "         'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "             'action': 'query',\n",
    "             'format': 'json',\n",
    "             'titles': titles,\n",
    "             'prop': 'extracts',\n",
    "             'exintro': True,\n",
    "             'explaintext': True,\n",
    "         }\n",
    "    ).json()\n",
    "    \n",
    "    try:\n",
    "        pages = response['query']['pages']\n",
    "\n",
    "        page_ids = list(pages.keys())\n",
    "\n",
    "        texts = [pages[x]['extract'] for x in page_ids]\n",
    "        names_all = [pages[x]['title'] for x in page_ids]\n",
    "    \n",
    "        df = pd.DataFrame({'Titles':names_all, 'Text':texts})\n",
    "        df = df.replace('', np.nan).dropna()\n",
    "        return df\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def collect_text(all_links):\n",
    "    \n",
    "    splits = split_into_20_names(all_links)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        df_all = get_text(splits[0])\n",
    "        if df_all is not None:\n",
    "\n",
    "            for i, v in enumerate(splits[1:]):\n",
    "                df = get_text(v)\n",
    "                df_all = df_all.append(df)\n",
    "                time.sleep(random.choice([.2,.1,.25,.5,.5,1, .75]))\n",
    "\n",
    "            return df_all\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Making the Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23001"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_edges = []\n",
    "for person in all_info['pldb']:\n",
    "    for connection in all_info['pldb'][person]:\n",
    "        \n",
    "        all_edges.append((person.split('(')[0].strip(), connection.split('(')[0].strip()))\n",
    "len(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edges.txt', 'w', encoding='utf8') as file:\n",
    "    for edge in all_edges:\n",
    "        file.write(str(edge) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.MultiDiGraph()\n",
    "G.add_edges_from(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9656"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9635"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_nodes = list(G.nodes()).copy()\n",
    "\n",
    "for node in bad_nodes:\n",
    "    if 'USS' in node:\n",
    "        G.remove_node(node)\n",
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22971"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_un = G.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Henry Steele Commager',\n",
       " 'George Washington',\n",
       " 'John Adams',\n",
       " 'John Sullivan',\n",
       " 'Richard Howe, 1st Earl Howe',\n",
       " 'Charles Gravier, comte de Vergennes',\n",
       " 'Benjamin Rush',\n",
       " 'John André',\n",
       " 'Sir Charles Asgill, 2nd Baronet',\n",
       " 'Friedrich Wilhelm von Steuben',\n",
       " 'Anthony Wayne']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.center(G_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Voltaire', 0.046688727694585964),\n",
       " ('George III of the United Kingdom', 0.04592306585004017),\n",
       " ('Benjamin Franklin', 0.04580874811855003),\n",
       " ('Isaac Newton', 0.04556568790568188),\n",
       " ('Thomas Jefferson', 0.04543710748125668),\n",
       " ('George Washington', 0.04519796481030269),\n",
       " ('John Adams', 0.045118809005030715),\n",
       " ('John Locke', 0.04435750678697062),\n",
       " ('Adam Smith', 0.043769690692953674),\n",
       " ('Napoleon', 0.0426961218791391)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close = nx.closeness_centrality(G)\n",
    "\n",
    "close = [(x, close[x]) for x in close]\n",
    "\n",
    "close.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "close[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Washington', 0.04068922565912394),\n",
       " ('Benjamin Franklin', 0.030932115424538095),\n",
       " ('John Adams', 0.026261158397342745),\n",
       " ('Charles I of England', 0.021175005189952252),\n",
       " ('Samuel Johnson', 0.021175005189952252),\n",
       " ('Ronald Reagan', 0.019514220469171686),\n",
       " ('Isaac Newton', 0.019099024288976543),\n",
       " ('Thomas Jefferson', 0.018995225243927757),\n",
       " ('Alexander Hamilton', 0.018268631928586257),\n",
       " ('George III of the United Kingdom', 0.017853435748391117)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = nx.degree_centrality(G)\n",
    "\n",
    "degree = [(x, degree[x]) for x in degree]\n",
    "\n",
    "degree.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "degree[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Benjamin Franklin', 0.004258852039587822),\n",
       " ('George Washington', 0.0030942355702121946),\n",
       " ('John Adams', 0.0018253441155465936),\n",
       " ('Charles I of England', 0.0016118857259612633),\n",
       " ('Voltaire', 0.0014673283474280691),\n",
       " ('Isaac Newton', 0.001347261165748261),\n",
       " ('Thomas Jefferson', 0.0012743157601116425),\n",
       " ('George III of the United Kingdom', 0.0011427073330807635),\n",
       " ('Adam Smith', 0.0010650674070461401),\n",
       " ('Alexander Pope', 0.0009820907826732358),\n",
       " ('Joseph Priestley', 0.000975978745771959),\n",
       " ('Louis XVI of France', 0.0008869650702429689),\n",
       " ('Charles II of England', 0.0007893976569688588),\n",
       " ('George II of Great Britain', 0.0007522205828167403),\n",
       " ('Cotton Mather', 0.0007268609955859174),\n",
       " ('Alexander Hamilton', 0.0007086608394314824),\n",
       " ('John Locke', 0.0006946819618646211),\n",
       " ('Louis XV of France', 0.0006742621587929328),\n",
       " ('Josiah Wedgwood', 0.0006578829561990841),\n",
       " ('Oliver Cromwell', 0.0006250278020396549)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_rank = nx.pagerank_scipy(G, alpha=.85)\n",
    "\n",
    "page_rank = [(x, page_rank[x]) for x in page_rank]\n",
    "\n",
    "page_rank.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "page_rank[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs, authorities = nx.hits_scipy(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Washington', 0.012711912980995223),\n",
       " ('John Marshall', 0.01224335952438163),\n",
       " ('Gilbert Stuart', 0.011139707560932395),\n",
       " ('American Enlightenment', 0.010863381867449413),\n",
       " ('George Wythe', 0.010667429002778206),\n",
       " ('Charles Cotesworth Pinckney', 0.009236835091633465),\n",
       " ('Thomas Jefferson', 0.008869746385669605),\n",
       " ('James Monroe', 0.00886284530607328),\n",
       " ('Benjamin Franklin', 0.008831807000495993),\n",
       " ('James Madison', 0.00871495408999184),\n",
       " ('Thomas Paine', 0.008670332000173003),\n",
       " ('Alexander Hamilton', 0.008431035435007502),\n",
       " ('Presidency of George Washington', 0.008340901283339662),\n",
       " ('John Adams', 0.008279419239278859),\n",
       " ('John Jay', 0.008042762596269015),\n",
       " ('Charles Willson Peale', 0.007865015393345375),\n",
       " ('John Quincy Adams', 0.007814395746957584),\n",
       " ('Thomas Pinckney', 0.0076949020269868286),\n",
       " ('Benjamin Franklin Bache', 0.007499862842723682),\n",
       " ('Oliver Ellsworth', 0.007372780842179773)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubs = [(x, hubs[x]) for x in hubs]\n",
    "\n",
    "hubs.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "hubs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Washington', 0.02824384838168282),\n",
       " ('Thomas Jefferson', 0.020908678701929057),\n",
       " ('John Adams', 0.020504506857274098),\n",
       " ('Benjamin Franklin', 0.01399312461806825),\n",
       " ('James Madison', 0.011696344224675378),\n",
       " ('Alexander Hamilton', 0.011469894196930674),\n",
       " ('John Jay', 0.006222607592483518),\n",
       " ('Joseph Priestley', 0.0056987902232181475),\n",
       " ('James Monroe', 0.00565130216981195),\n",
       " ('Aaron Burr', 0.005174133421846327),\n",
       " ('John Marshall', 0.005106121312618697),\n",
       " ('Voltaire', 0.004788902318911247),\n",
       " ('John Locke', 0.004769312500282762),\n",
       " ('Andrew Jackson', 0.004566562079045171),\n",
       " ('George III of the United Kingdom', 0.004201124482345451),\n",
       " ('John Quincy Adams', 0.004136507786944289),\n",
       " ('Thomas Paine', 0.004022665739271341),\n",
       " ('Horatio Gates', 0.0036565251791828646),\n",
       " ('Patrick Henry', 0.0035848955560802216),\n",
       " ('Edmund Burke', 0.0035811012962293698)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorities = [(x, authorities[x]) for x in authorities]\n",
    "\n",
    "authorities.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "authorities[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotly\\plotly\\plotly.py:233: UserWarning:\n",
      "\n",
      "Woah there! Look at all those points! Due to browser limitations, the Plotly SVG drawing functions have a hard time graphing more than 500k data points for line charts, or 40k points for other types of charts. Here are some suggestions:\n",
      "(1) Use the `plotly.graph_objs.Scattergl` trace object to generate a WebGl graph.\n",
      "(2) Trying using the image API to return an image instead of a graph URL\n",
      "(3) Use matplotlib\n",
      "(4) See if you can create your visualization with fewer data points\n",
      "\n",
      "If the visualization you're using aggregates points (e.g., box plot, histogram, etc.) you can disregard this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The draw time for this plot will be slow for clients without much RAM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plotly\\api\\v1\\clientresp.py:40: UserWarning:\n",
      "\n",
      "Estimated Draw Time Slow\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~forresthangen/6.embed\" height=\"1000px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_3d(G_un, 'Large Wikipedia Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"C:\\\\Users\\\\forre\\\\Documents\\\\Python\\\\Data Science Projects\\\\Web Scraping Practice\\\\Secret\\\\cred.txt\", header = None)\n",
    "\n",
    "plotly.tools.set_credentials_file(username=temp[0][0].strip(), api_key=temp[1][0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_3d(G_un, filename):    \n",
    "    nodes_dict = {}\n",
    "    for i, v in enumerate(list(G_un.nodes())):\n",
    "        nodes_dict[v] = i\n",
    "\n",
    "    Edges = [(nodes_dict[x[0]], nodes_dict[x[1]]) for x in list(G_un.edges())]\n",
    "\n",
    "    G_ig_un = ig.Graph(Edges, directed = False)\n",
    "\n",
    "    layt=G_ig_un.layout('fr_3d', dim=3)\n",
    "\n",
    "    labels= list(G_un.nodes())\n",
    "    labels = [str(x) for x in labels]\n",
    "\n",
    "    group = []\n",
    "\n",
    "    for i in dict(nx.degree(G_un)):\n",
    "        group.append(dict(nx.degree(G_un))[i])\n",
    "        \n",
    "    ma = max(group)\n",
    "    mi = min(group)\n",
    "    \n",
    "    def equalizer(x):\n",
    "        new_val = ((80-2)*(x-mi))/(ma-mi)\n",
    "        return new_val\n",
    "    group = [equalizer(x) for x in group]\n",
    "\n",
    "    N = len(labels)\n",
    "\n",
    "    Xn=[layt[k][0] for k in range(N)]# x-coordinates of nodes\n",
    "    Yn=[layt[k][1] for k in range(N)]# y-coordinates\n",
    "    Zn=[layt[k][2] for k in range(N)]# z-coordinates\n",
    "    Xe=[]\n",
    "    Ye=[]\n",
    "    Ze=[]\n",
    "    for e in Edges:\n",
    "        Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends\n",
    "        Ye+=[layt[e[0]][1],layt[e[1]][1], None]\n",
    "        Ze+=[layt[e[0]][2],layt[e[1]][2], None]\n",
    "\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace1=go.Scatter3d(x=Xe,\n",
    "                   y=Ye,\n",
    "                   z=Ze,\n",
    "                   mode='lines',\n",
    "                   line=dict(color='rgb(125,125,125)', width=1),\n",
    "                   hoverinfo='none'\n",
    "                   )\n",
    "    trace2=go.Scatter3d(x=Xn,\n",
    "                   y=Yn,\n",
    "                   z=Zn,\n",
    "                   mode='markers',\n",
    "                   name='actors',\n",
    "                   marker=dict(symbol='circle',\n",
    "                                 size=[x+6 for x in group],\n",
    "                                 color=group,\n",
    "                                 colorscale='Bluered',\n",
    "                                 line=dict(color='rgb(50,50,50)', width=0.5)\n",
    "                                 ),\n",
    "                   text=labels,\n",
    "                   hoverinfo='text'\n",
    "                   )\n",
    "    axis=dict(showbackground=False,\n",
    "              showline=False,\n",
    "              zeroline=False,\n",
    "              showgrid=False,\n",
    "              showticklabels=False,\n",
    "              title=''\n",
    "              )\n",
    "    layout = go.Layout(\n",
    "             title=\"Benjamin Franklin's Wikipedia Network (3D visualization)\",\n",
    "             width=1000,\n",
    "             height=1000,\n",
    "             showlegend=False,\n",
    "             scene=dict(\n",
    "                 xaxis=dict(axis),\n",
    "                 yaxis=dict(axis),\n",
    "                 zaxis=dict(axis),\n",
    "            ),\n",
    "         margin=dict(\n",
    "            t=100\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        annotations=[\n",
    "               dict(\n",
    "               showarrow=False,\n",
    "                text=\"Data source: Wikipedia\",\n",
    "                xref='paper',\n",
    "                yref='paper',\n",
    "                x=0,\n",
    "                y=0.1,\n",
    "                xanchor='left',\n",
    "                yanchor='bottom',\n",
    "                font=dict(\n",
    "                size=14\n",
    "                )\n",
    "                )\n",
    "            ],    )\n",
    "    data=[trace1, trace2]\n",
    "    fig=go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename = filename, auto_open = True, world_readable = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
